## 停机维护

------

停机维护前需要先关闭重平衡相关的操作，防止在停机维护过程中进行大量数据平衡，命令如下

```
`sh /usr/local/bin/ceph-devops/stop-rebalance.sh  # 在任意ceph服务器上执行该命令即可，该命令是集群级别的

# 验证方法 ceph -s | grep nobackfill |grep norecover | grep norebalance | grep flag(s) set # 返回有结果表明ok`
```

然后关闭需要停机维护服务器上的所有ceph组件

```
`systemctl stop ceph-*.target  # 在需要停机的服务器上执行

 # 验证方法 ps aux | grep -v grep | grep -E '(ceph-mon|ceph-osd|ceph-mds|ceph-mgr|radosgw)' # 没有对应的进程表示服务都已经停止了`
```

## 新机器安装文档

------

新装服务器主要分为两步，第一步是创建用户和安装基础程序启动进程等操作，第二步下发ceph集群配置到新机器，第三步初始化硬盘和创建osd进程

```
`sh /usr/local/bin/ceph-devops/deploy-server.sh # 该命令在新装服务器上运行

 # 验证方法 ceph --version # 返回ceph版本

ps aux | grep node_exporter # 有进程

route -n # 有对应容器网段的路由，10.20x.0.0`
```



```
`# 第二步需要在deploy服务器上运行，vlan1的deploy服务器是192.168.215.89

 vim /etc/hosts #增加新服务器的hosts
su - cephfsd

ceph-deploy --overwrite-conf admin $hostname

 # 验证方法
ceph -s # 在目标机器执行返回成功表示已经好了`
```



注：如果不需要安装osd数据盘，第三步不需要执行

```
`# 第三步需要在deploy服务器上运行，vlan1的deploy服务器是192.168.215.89

su - cephfsd

 sh /usr/local/bin/ceph-devops/deploy-osd.sh $hostname sdb,sdc,sde,...sdn # 指定目标机器及对应的块设备编号

# 验证方法
ceph osd status | grep $hostname # 确认最后一列是 exists,up表示osd已经启动完毕

# 由于现在测试和vlan1的ceph都变为了rack为故障域，并且线上还没有开启自动加入osd，所以需要手动的将新的bucket加入进去，命令如下

 ceph osd crush add-bucket $hostname host # 该命令是新增一个host级的bucket，名称是$hostname，一般以目标服务器的hostname作为名称

 ceph osd crush add osd.* $weight host=$hostname # 该命令是将osd加入到host中去，如果已经自动创建了这一步不需要，$weight为硬盘容量转换为T的数值，

 ceph osd crush move $hostname rack=$rackname # 该命令是把整个host移动到对应的rack下面 `
```

## 部署对象网关

------

```
`ceph-deploy rgw create $node1 $node2 $node  # 验证方法 ceph -s | grep rgw # 确认有对应节点的实例名称即表示安装成功`
```

## 恢复服务器上已经存在的osd盘

------

该流程主要是在已经有数据盘的服务器重装时，或者将数据盘迁移到其他服务器时的操作流程，在重装前和数据盘迁移前不需要前置操作

```
`vgs # 列出当前的vg组 vgimport $vgid # 导入需要修复的vg组，vgid为上一步第一列数据 vgchange -ay $vgid # 激活vg组 lvs -o lv_tags | grep $vgid # 查看该vg对应的osd标签，需要查询到osd的编号和fsid，找到如下两个数据ceph.osd_id=$osdNum, ceph.osd_fsid=$fsid,  ceph-volume lvm activate $osdNum $fsid ceph osd in $osdNum`
```

## 对象存储开通账号和bucket

------

开通账号和增加bucket是两步，命令如下：

```
`# 开通账号【执行完后会展示密钥，复制发给业务方】 /usr/local/bin/ceph-devops/rgw-mkuser.sh -u $username -m $maxsize # $username为申请邮件中的用户名，$maxsize为邮件中该用户能够存储的最大容量  # 给某账号加bucket /usr/local/bin/ceph-devops/rgw-mkbucket.sh -u $username -p $bucketname -n $sharding # $username为上一步名称，$bucketname为申请邮件中的bucket，注意有些用户发送的时候$bucketname里包含了用户名($username.$bucketname)，需要去掉，只能有$bucketname，$sharding为用户指定的分片数  # 给某bucket开通公开权限（开通后该bucket的所有key可以直接使用http请求下载，不需要密钥） /usr/local/bin/ceph-devops/rgw-mkbucket-public.sh $username $bucketname # 注意$username和$bucketname是分开的，不能用$username.$bucketname  # 给老的s3增加bucket /usr/local/bin/ceph-devops/rgw-mkswiftbucket.sh $s3accessKey $bucket $isPublic # $s3accessKey需要业务方提供老的s3 access_key，$bucket是需要新创建的bucket名称，$ispublic表示是否公开该bucket  `
```

## 存储池扩/缩pg

------

在某些场景下需要对存储池(pool)进行扩/缩pg，例如：当集群规模变大，扩容服务器或者osd时，扩pg是为了让每个osd上的pg足够均匀，防止某些osd过大出现木桶效应，pg的算法见https://www.cnblogs.com/iouwenbo/p/12955945.html

一般来说主要是由两个因素决定：

1. 对应存储的容量占当前集群的容量的占比
2. 当前osd pg数均匀程度

当系统出现osd极度不均匀是就需要对一些pool的pg数进行扩/缩容了。

首先看平均osd的pg数，是否非常不均匀，如果是则需要对容量最大的pool进行扩容，一般都是翻倍，保证pg和pgp是2^n，如果数据量比较大，建议先扩到1.5倍，然后再扩到2倍，平滑一些

如果osd相对已经平均了，而osd的容量仍然是不均匀则可能是有些容量很小的pool占用了大量pg，需要将这些pg缩容

```
`ceph osd pool ls detail # 该命令用于查看现有存储所设置的pg和pgp数量 ceph df # 用于查看每个存储的容量 ceph osd pool set $poolname $pg && ceph osd pool set $poolname pgp_num $pgp # $poolname为存储池名称，$pg和$pgp应该设置为一致`
```



## 容器块存储扩容

\#扩容:

rbd resize kube1/deploycmas137 --size 30T

\#查看扩容结果：

rbd info kube1/deploycmas137



\#查看块大小：

```
blockdev --getsize64 /dev/rbd0#容器那边文件系统扩容： resize2fs /dev/rbd0
```

## multipart过期清理

如果在上传对象过程中因异常原因中断，此情况会残留大量的multipart，需要对bucket设置残留multipart的过期清理策略，策略配置如下：

lifecyclePolicy

```
`<?xml version="1.0" ?>   <LifecycleConfiguration xmlns="http://s3.amazonaws.com/doc/2006-03-01/">     <Rule>         <ID>IncompleteMultipartUploads</ID>           <Prefix/>             <Status>Enabled</Status>              <AbortIncompleteMultipartUpload>                 <DaysAfterInitiation>1</DaysAfterInitiation>             </AbortIncompleteMultipartUpload>     </Rule>   </LifecycleConfiguration>`
```

执行命令如下：

 s3cmd setpolicy lifecyclePolicy s3://BUCKET

## 对象索引清理

通过s3cmd ls查看的对象数和实际rgw中保存的对象数不一致，出现对象索引残留的情况，现象如：



清理命令：

```
`radosgw-admin bucket check --check-objects --fix --bucket <bucket name>`
```
